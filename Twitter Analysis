import requests
import pandas as pd

api_key = "4815e239fd0c03999e8f272f0f79f2ea"

query = "VAR"

total_tweets = 4000

num_per_request = 100

# Number of tweets to process in each iteration
chunk_size = 1000

# Create an empty list to store all retrieved tweets
all_tweets = []

# Make multiple API calls to retrieve the specified number of tweets
for i in range(0, total_tweets, num_per_request):
    payload = {
        "api_key": api_key,
        "query": query,
        "num": min(num_per_request, total_tweets - i), # Adjust for the last chunk
        "start": i
    }

    response = requests.get("http://api.scraperapi.com/structured/twitter/search", params=payload)

    data = response.json()

    # Add retrieved tweets to the list
    all_tweets.extend(data["organic_results"])

    # Process tweets in chunks
    if len(all_tweets) >= chunk_size:
        # Create a DataFrame from the list of tweets
        df = pd.DataFrame(all_tweets)

        # Save the DataFrame to CSV (adjust the file path accordingly)
        df.to_csv('twitter_data.csv', mode='a', index=False, header=not i)

        # Clear the list for the next chunk
        all_tweets = []

# Process the remaining tweets
if all_tweets:
    df = pd.DataFrame(all_tweets)
    df.to_csv('twitter_data.csv', mode='a', index=False, header=False)
# Load the saved CSV file if needed
final_df = pd.read_csv('twitter_data.csv')
